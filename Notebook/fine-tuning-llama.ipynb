{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ee2753-bb2e-46a8-bd7c-47f5dfc2c46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries for language model fine-tuning and inference\n",
    "!pip install unsloth  # Core library for model management\n",
    "!pip install torch  # PyTorch library for GPU-accelerated training and inference\n",
    "!pip install transformers  # Hugging Face library for NLP model handling\n",
    "!pip install datasets  # Library for loading and managing datasets\n",
    "!pip install trl  # Library for training reinforcement learning-based NLP models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd35f72-d60f-4bae-b6f0-9ba07f625f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules for model management, dataset handling, and fine-tuning\n",
    "import torch\n",
    "from unsloth import FastLanguageModel  # High-performance language model utilities\n",
    "from unsloth.chat_templates import get_chat_template",
    "from datasets import load_dataset  # For loading datasets\n",
    "from trl import SFTTrainer  # Supervised fine-tuning trainer\n",
    "from transformers import TrainingArguments  # Configuration for training process\n",
    "from unsloth.chat_templates import get_chat_template, standardize_sharegpt  # Chat template utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24135ce-63db-4f50-9249-6cbad936dc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained Llama-3.2 model with 3 billion parameters, optimized for instruction following\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=True,  # Use 4-bit precision for efficient memory usage\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb056af6-58b5-4c1e-9b66-b3c5fd6a6d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Parameter-Efficient Fine-Tuning (PEFT) to reduce training resource requirements\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  # Bottleneck dimension for fine-tuning\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention projection layers\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",  # Feedforward layers\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1534e5-3771-4a68-a355-ab0a51db600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure tokenizer with chat-style templates for input-output formatting\n",
    "tokenizer = get_chat_template(tokenizer, chat_template=\"llama-3.1\")\n",
    "\n",
    "# Load the dataset and select the training split\n",
    "dataset = load_dataset(\"mlabonne/FineTome-100k\", split=\"train\")\n",
    "\n",
    "# Standardize the dataset using ShareGPT format and prepare input text using templates\n",
    "dataset = standardize_sharegpt(dataset)\n",
    "dataset = dataset.map(\n",
    "    lambda examples: {\n",
    "        \"text\": [\n",
    "            tokenizer.apply_chat_template(convo, tokenize=False)\n",
    "            for convo in examples[\"conversations\"]\n",
    "        ]\n",
    "    },\n",
    "    batched=True,  # Process in batches for efficiency\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c9956d-3763-4d07-b7b5-4b065aced9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the fine-tuning trainer with training arguments\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=2048,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,  # Batch size per device\n",
    "        gradient_accumulation_steps=4,  # Accumulate gradients over 4 steps\n",
    "        warmup_steps=5,  # Warm-up steps for learning rate\n",
    "        max_steps=60,  # Total training steps\n",
    "        learning_rate=2e-4,  # Learning rate\n",
    "        fp16=not torch.cuda.is_bf16_supported(),  # Use FP16 if BF16 not supported\n",
    "        bf16=torch.cuda.is_bf16_supported(),  # Use BF16 if supported\n",
    "        logging_steps=1,  # Log every step\n",
    "        output_dir=\"outputs\",  # Directory for saving outputs\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b769604-b521-4a21-bffc-559aaba31fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform fine-tuning on the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model to the specified directory\n",
    "model.save_pretrained(\"finetuned_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33a1c3b-338e-428d-9d44-57cef92c9bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model for inference\n",
    "model_path = \"finetuned_model\"  # Directory of the fine-tuned model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_path,\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8378eb55-8bae-487f-8d7f-7000cb6640af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable faster inference using optimized settings\n",
    "FastLanguageModel.for_inference(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fe18d8-3a80-493b-9392-d2f6d99fd234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules for inference\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_path = \"finetuned_model\"\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_path,\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=True,  # Use 4-bit precision\n",
    ")\n",
    "\n",
    "# Enable optimized inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Determine the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  # Move the model to the appropriate device\n",
    "\n",
    "# Define an input prompt for text generation\n",
    "input_prompt = \"\"\"I am 25 years old, earning $50,000 per year. I have $5,000 in credit card \n",
    "    debt at 18% interest, $10,000 in student loans at 5% interest, and $2,000 in savings. \n",
    "    I want to buy a house within the next 5 years, but I also need to save for retirement. \n",
    "    How should I prioritize paying off my debt, saving for a down payment, and investing \n",
    "    for retirement?\"\"\"\n",
    "\n",
    "# Tokenize the input prompt with padding and truncation\n",
    "inputs = tokenizer(\n",
    "    input_prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=2048\n",
    ").to(device)\n",
    "\n",
    "# Ensure model and inputs are on the same device\n",
    "assert model.device == inputs[\"input_ids\"].device, \"Model and inputs are on different devices.\"\n",
    "\n",
    "# Generate text without gradient computation\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=200,  # Maximum length of generated text\n",
    "        num_return_sequences=1,  # Generate a single output sequence\n",
    "        temperature=0.7,  # Sampling temperature\n",
    "        top_p=0.9,  # Top-p nucleus sampling\n",
    "    )\n",
    "\n",
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
